import os
import time
import pandas as pd
from io import StringIO
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By

# Load player list
df = pd.read_excel("AFL_Fantasy_Player_URLs.xlsx")

# Setup headless Chrome
options = Options()
options.add_argument("--headless")
options.add_argument("--disable-gpu")
options.add_argument("--no-sandbox")
options.add_argument("--disable-dev-shm-usage")
service = Service()
driver = webdriver.Chrome(service=service, options=options)

# Output folder
output_folder = "dfs_player_summary"
os.makedirs(output_folder, exist_ok=True)

# Table IDs to extract
TABLE_IDS = {
    "Career Averages": "fantasyPlayerCareer",
    "Opponent Splits": "vsOpponentCareer",
    "Game Logs": "playerGames"
}

# Scrape loop
for index, row in df.iterrows():
    player_id = row["playerId"]
    url = row["url"]
    print(f"ðŸ”„ Scraping {player_id}...")

    output_path = os.path.join(output_folder, f"{player_id}.xlsx")

    # Delete existing file
    if os.path.exists(output_path):
        try:
            os.remove(output_path)
            